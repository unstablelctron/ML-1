{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfbb44ce",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q1\"></a>Question 1: Explain the differences between AI, ML, Deep Learning (DL), and Data Science (DS).\n",
    "\n",
    "**Answer:**  \n",
    "- **Artificial Intelligence (AI):** The broad field of building systems that can perform tasks that normally require human intelligence (reasoning, planning, perception, language understanding, decision-making). AI includes rule-based systems, search/optimization, and learning-based systems.  \n",
    "- **Machine Learning (ML):** A subfield of AI focused on algorithms that **learn patterns from data** to make predictions or decisions without being explicitly programmed with rules. Examples: linear/logistic regression, decision trees, random forests, SVMs, gradient boosting.  \n",
    "- **Deep Learning (DL):** A subfield of ML that uses **neural networks with many layers** to automatically learn hierarchical representations (features). Particularly powerful for unstructured data like images, audio, and text (e.g., CNNs, RNNs, Transformers).  \n",
    "- **Data Science (DS):** An interdisciplinary field that combines **statistics, programming, and domain knowledge** to extract insights and value from data. DS spans the entire lifecycle: data collection, cleaning, EDA, visualization, modeling (often using ML), evaluation, and communication of results.\n",
    "\n",
    "**Relationship:** AI ⊇ ML ⊇ DL; DS overlaps with ML/DL but also includes data engineering, analytics, and communication.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49be33ae",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q2\"></a>Question 2: What are the types of machine learning? Describe each with one real-world example.\n",
    "\n",
    "**Answer:**  \n",
    "1. **Supervised Learning:** Learn a mapping from inputs to a **labeled** target (regression/classification).  \n",
    "   - *Example:* Predicting house prices (regression) using features like area, location, and rooms.  \n",
    "2. **Unsupervised Learning:** Discover structure in **unlabeled** data (clustering, dimensionality reduction).  \n",
    "   - *Example:* Customer segmentation using purchase behavior (k-means).  \n",
    "3. **Semi-supervised Learning:** Train on a small amount of labeled data plus a large amount of unlabeled data.  \n",
    "   - *Example:* Classifying webpages when only a subset is labeled.  \n",
    "4. **Self-supervised Learning:** Create proxy labels from the data itself to pretrain representations.  \n",
    "   - *Example:* Masked language modeling to pretrain text encoders.  \n",
    "5. **Reinforcement Learning:** An agent learns to act by receiving **rewards** from an environment.  \n",
    "   - *Example:* Game-playing agents (e.g., learning to play Atari or Go).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b458595f",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q3\"></a>Question 3: Define overfitting, underfitting, and the bias-variance tradeoff in machine learning.\n",
    "\n",
    "**Answer:**  \n",
    "- **Overfitting:** The model fits training data too closely (including noise), yielding **low training error** but **high validation/test error**.  \n",
    "- **Underfitting:** The model is too simple to capture underlying patterns, giving **high training and test error**.  \n",
    "- **Bias–Variance Tradeoff:**  \n",
    "  - **Bias** = error due to simplifying assumptions (too simple → underfit).  \n",
    "  - **Variance** = error due to sensitivity to training fluctuations (too complex → overfit).  \n",
    "  Managing model complexity, adding more data, using regularization/ensembles, and proper validation helps balance the two.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ec9ccc",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q4\"></a>Question 4: What are outliers in a dataset, and list three common techniques for handling them.\n",
    "\n",
    "**Answer:**  \n",
    "- **Outliers** are observations that deviate markedly from other observations and may arise from data entry errors, rare events, or natural variability.  \n",
    "- **Common handling techniques:**  \n",
    "  1. **Capping/Winsorization:** Replace extreme values beyond certain percentiles (e.g., 1st/99th).  \n",
    "  2. **Transformation:** Apply log/Box–Cox/Yeo–Johnson to reduce skewness and compress extremes.  \n",
    "  3. **Removal/Filtering:** Drop points beyond thresholds (e.g., IQR method or z-score).  \n",
    "  4. **Robust Models:** Use algorithms or losses robust to outliers (e.g., median-based stats, Huber loss).  \n",
    "  5. **Imputation by Domain Logic:** If outliers are due to errors, correct with valid values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc45879",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q5\"></a>Question 5: Explain the process of handling missing values and mention one imputation technique for numerical and one for categorical data.\n",
    "\n",
    "**Answer:**  \n",
    "1. **Explore missingness:** quantify % missing per feature; analyze patterns (MCAR/MAR/MNAR).  \n",
    "2. **Decide strategy per feature:** drop columns/rows (when justified) vs. impute.  \n",
    "3. **Impute appropriately:**  \n",
    "   - **Numerical:** mean/median imputation, KNN imputation, model-based, or time-series interpolation.  \n",
    "   - **Categorical:** most frequent (mode), introduce **'Unknown'** category, or model-based.  \n",
    "4. **Flag imputation:** add indicator columns where appropriate.  \n",
    "5. **Validate impact:** compare model performance with/without different strategies.\n",
    "\n",
    "**Example techniques:**  \n",
    "- Numerical → **Median imputation** (robust to outliers).  \n",
    "- Categorical → **Most frequent (mode)** or **'Unknown'** label.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620bef64",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q6\"></a>Question 6: Write a Python program that creates a synthetic imbalanced dataset with `make_classification()` and prints the class distribution.\n",
    "\n",
    "**Answer (Code):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7337a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.datasets import make_classification\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_samples=2000, n_features=10, n_informative=3, n_redundant=2,\n",
    "    n_clusters_per_class=1, weights=[0.95, 0.05], flip_y=0.01, random_state=42\n",
    ")\n",
    "\n",
    "print(\"Shapes:\", X.shape, np.array(y).shape)\n",
    "print(\"Class distribution:\", Counter(y))\n",
    "\n",
    "# Optional: show as a DataFrame head\n",
    "df = pd.DataFrame(X, columns=[f\"f{i}\" for i in range(X.shape[1])])\n",
    "df['target'] = y\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff32e4a",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q7\"></a>Question 7: Implement one-hot encoding using pandas for the list of colors `['Red', 'Green', 'Blue', 'Green', 'Red']` and print the resulting DataFrame.\n",
    "\n",
    "**Answer (Code):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647891e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "colors = ['Red', 'Green', 'Blue', 'Green', 'Red']\n",
    "df_colors = pd.DataFrame({'color': colors})\n",
    "encoded = pd.get_dummies(df_colors, columns=['color'], prefix='color')\n",
    "\n",
    "print(\"Original:\")\n",
    "print(df_colors)\n",
    "print(\"\\nOne-hot encoded:\")\n",
    "print(encoded)\n",
    "encoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954c562c",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q8\"></a>Question 8: Generate 1000 normal samples, introduce 50 random missing values, fill with mean, and plot histograms before and after imputation.\n",
    "\n",
    "**Answer (Code):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0b41a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate data\n",
    "data = np.random.normal(loc=0, scale=1, size=1000).astype(float)\n",
    "data_with_nan = data.copy()\n",
    "\n",
    "# Introduce 50 random missing values\n",
    "nan_indices = np.random.choice(len(data_with_nan), size=50, replace=False)\n",
    "data_with_nan[nan_indices] = np.nan\n",
    "\n",
    "# Impute with mean\n",
    "mean_value = np.nanmean(data_with_nan)\n",
    "imputed = np.where(np.isnan(data_with_nan), mean_value, data_with_nan)\n",
    "\n",
    "print(f\"Mean used for imputation: {mean_value:.4f}\")\n",
    "print(\"Missing before:\", np.isnan(data_with_nan).sum(), \"| Missing after:\", np.isnan(imputed).sum())\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure()\n",
    "plt.hist(data_with_nan[~np.isnan(data_with_nan)], bins=30, alpha=0.8)\n",
    "plt.title(\"Histogram BEFORE imputation\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(imputed, bins=30, alpha=0.8)\n",
    "plt.title(\"Histogram AFTER mean imputation\")\n",
    "plt.xlabel(\"Value\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2acd6a6",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q9\"></a>Question 9: Implement Min–Max scaling on `[2, 5, 10, 15, 20]` using `sklearn.preprocessing.MinMaxScaler` and print the scaled array.\n",
    "\n",
    "**Answer (Code):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([[2], [5], [10], [15], [20]], dtype=float)\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(arr)\n",
    "\n",
    "print(\"Original:\", arr.ravel().tolist())\n",
    "print(\"Scaled:\", scaled.ravel().tolist())\n",
    "scaled\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de2538f",
   "metadata": {},
   "source": [
    "\n",
    "### <a id=\"q10\"></a>Question 10: Data preparation plan for retail transactions (missing ages, outliers in amount, imbalanced target, categorical variables).\n",
    "\n",
    "**Answer:**  \n",
    "**Step-by-step plan:**  \n",
    "1. **Schema & sanity checks:** Validate columns, dtypes, duplicate rows, impossible values (e.g., negative ages).  \n",
    "2. **Missing ages:**  \n",
    "   - Explore age distribution and correlation with other features.  \n",
    "   - Impute with **median** (robust) or **KNN imputer** using related features; add `age_imputed` flag.  \n",
    "3. **Outliers in transaction amount:**  \n",
    "   - Identify via **IQR method** or log-transform.  \n",
    "   - Handle via **capping** at reasonable percentiles (e.g., 1%/99%) or use **robust scalers**.  \n",
    "4. **Imbalanced target (fraud vs non-fraud):**  \n",
    "   - Use **stratified splits**, evaluate with **PR-AUC**, **F1**, **recall**.  \n",
    "   - Apply **class weighting**, **SMOTE/SMOTEENN**, or **threshold tuning**.  \n",
    "5. **Categorical variables (e.g., payment method):**  \n",
    "   - Low-cardinality → **one-hot**; high-cardinality → **target encoding** with CV to avoid leakage.  \n",
    "6. **Scaling:**  \n",
    "   - Use **StandardScaler/RobustScaler** for numeric features; fit on train only.  \n",
    "7. **Validation & leakage control:**  \n",
    "   - Pipeline + ColumnTransformer; fit only on training folds; use nested CV if tuning.  \n",
    "8. **Modeling:**  \n",
    "   - Start with **logistic regression** (with class_weight) and **tree-based** baselines; calibrate probabilities if needed.  \n",
    "9. **Monitoring:**  \n",
    "   - Track data drift and alert on population changes (PSI), retrain periodically.\n",
    "\n",
    "**Optional (Code Template – synthetic demonstration):**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75332f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, RobustScaler\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# --- Synthetic dataset (for demo only) ---\n",
    "np.random.seed(42)\n",
    "n = 3000\n",
    "df = pd.DataFrame({\n",
    "    \"age\": np.random.normal(35, 12, n).round(0),\n",
    "    \"amount\": np.random.lognormal(mean=3.2, sigma=1.0, size=n),\n",
    "    \"payment_method\": np.random.choice([\"card\", \"upi\", \"netbanking\", \"cod\"], size=n, p=[0.6, 0.25, 0.1, 0.05]),\n",
    "})\n",
    "\n",
    "# Introduce missing ages (~10%)\n",
    "mask = np.random.rand(n) < 0.1\n",
    "df.loc[mask, \"age\"] = np.nan\n",
    "\n",
    "# Introduce some extreme outliers in amount\n",
    "outlier_idx = np.random.choice(n, size=20, replace=False)\n",
    "df.loc[outlier_idx, \"amount\"] *= 50\n",
    "\n",
    "# Imbalanced target (fraud 3%)\n",
    "y = (np.random.rand(n) < 0.03).astype(int)\n",
    "\n",
    "# --- Train/Valid split ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# --- Preprocessing ---\n",
    "numeric_features = [\"age\", \"amount\"]\n",
    "categorical_features = [\"payment_method\"]\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),       # or KNNImputer()\n",
    "    (\"scaler\", RobustScaler())                           # robust to outliers\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_transformer, numeric_features),\n",
    "        (\"cat\", categorical_transformer, categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# --- Modeling with imbalance handling ---\n",
    "model = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
    "\n",
    "pipeline = ImbPipeline(steps=[\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"smote\", SMOTE(sampling_strategy=0.2, random_state=42)),  # upsample minority in training folds\n",
    "    (\"clf\", model)\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Train score:\", pipeline.score(X_train, y_train))\n",
    "print(\"Test score:\", pipeline.score(X_test, y_test))\n",
    "\n",
    "# Inspect one prediction batch\n",
    "preds = pipeline.predict_proba(X_test)[:5, 1]\n",
    "pd.DataFrame({\"prob_fraud\": preds})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fdf45d",
   "metadata": {},
   "source": [
    "---\n",
    "*End of assignment solutions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
